name: Weekly Regression
on:
  schedule:
    - cron:  '0 1 * * MON' # runs at 2am CET MONDAY
  workflow_dispatch:
  repository_dispatch:
  push:
    branches:
      - '**'
      - '!main'
    paths-ignore:
      - '**'
      - '!.github/workflows/WeeklyRegression.yml'

  pull_request:
    types: [opened, reopened, ready_for_review]
    paths-ignore:
      - '**'
      - '!.github/workflows/WeeklyRegression.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/main' || github.sha }}
  cancel-in-progress: true

env:
  GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  instance_id: i-08880cd29bfcd3e61
  gh_issue_repo: Tmonster/test-ec2-github-action-runner

jobs:
  start-runner:
    name: Start self-hosted ec2 runner
    runs-on: ubuntu-latest
  
    steps:
      - name: Start EC2 runner
        shell: bash
        env:
          AWS_ACCESS_KEY_ID: ${{secrets.AWS_ACCESS_KEY_ID}}
          AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_SECRET_ACCESS_KEY}}
          AWS_DEFAULT_REGION: us-east-1
        run: aws ec2 start-instances --instance-id ${{ env.instance_id }}

      - name: Create issue if failure
        if: failure()
        shell: bash
        run: |
          gh issue create --repo ${{ env.gh_issue_repo }} --title "Weekly Regression Test Failure" --body "AWS box with instance-id ${{ env.instance_id }} could not be started"


  setup-benchmarks-on-runner:
    name: Setup Benchmarks
    needs: 
      - start-runner
    runs-on: self-hosted
    env:
      GEN: ninja
      BUILD_BENCHMARK: 1
      BUILD_TPCH: 1
      BUILD_TPCDS: 1
      BUILD_JEMALLOC: 1
      regression_output: regression_output.txt
      
    steps: 
      - name: Install
        shell: bash
        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq g++ ninja-build awscli cmake make python-is-python3 libssl-dev pip

      - name: Install requests
        shell: bash
        run: python3 -m pip install requests

      - name: umount duckdb-main (helps with debugging)
        shell: bash
        run: |
          if [ ! -d duckdb-main ] && [ ! -d duckdb-main/duckdb_benchmark_data ] ; then 
            exit 0;
          fi 
          if mountpoint -q duckdb-main/duckdb_benchmark_data ; then
            # unmount main duckdb_benchmark_data. During debugging the mount can cause steps
            # to fail when copying duckdb-main to duckdb-old
            sudo umount duckdb-main/duckdb_benchmark_data
          fi

      - name: checkout duckdb-main
        shell: bash
        run: |
          if [ ! -d duckdb-main ] ; then 
            git clone https://github.com/duckdb/duckdb duckdb-main
          else
            cd duckdb-main && git fetch --all && git checkout main && git pull origin main
          fi
          mkdir -p duckdb-main/duckdb_benchmark_data


      - name: checkout duckdb-old
        shell: bash
        run: |
          if [ ! -d duckdb-old ] ; then 
            git clone https://github.com/duckdb/duckdb duckdb-old
          else
            cd duckdb-old && git fetch --all && cd ..
          fi
          mkdir -p duckdb-old/duckdb_benchmark_data


      - name: Set duckdb versions
        shell: bash
        run: |
          # if main_version.txt exists and previous_failed.txt does not exist
          if [ ! -f previous_failed.txt ] && [ -f duckdb_main_version.txt ]; then 
            cd duckdb-old && git checkout $( cat ../duckdb_main_version.txt ) && cd ..
          fi

      - name: Update duckdb_main_version
        shell: bash
        working-directory: duckdb-main
        run: |
          # update duckdb_main_version.txt
          git rev-parse --verify HEAD > ../duckdb_main_version.txt

      - name: Build old and main
        shell: bash
        run: |
          cd duckdb-main && make clean && make 
          cd ..
          cd duckdb-old && make clean && make

      - name: mount duckdb_benchmark_data to persistent storage
        shell: bash
        working-directory: duckdb-main
        run: |
          rm -rf duckdb_benchmark_data
          sudo mkfs -t xfs -f /dev/nvme1n1
          mkdir duckdb_benchmark_data
          sudo mount /dev/nvme1n1 duckdb_benchmark_data
          sudo chown -R ubuntu duckdb_benchmark_data

      - name: Set up benchmarks
        shell: bash
        working-directory: duckdb-main
        run: |
          cp -r benchmark ../duckdb-old

      - name: Load data for sf100 benchmarks.
        shell: bash
        run: |
          wget https://duckdb-blobs.s3.amazonaws.com/data/tpch-sf100.db --output-document duckdb-main/duckdb_benchmark_data/tpch_sf100.duckdb

      - name: Link duckdb-old/duckdb_benchmark_data to duckdb-main/duckdb_benchmark_data
        working-directory: duckdb-old/duckdb_benchmark_data
        shell: bash 
        run: |
          rm -rf *
          ln -s ../../duckdb-main/duckdb_benchmark_data/tpch_sf100.duckdb .

  run-large-benchmarks-on-runner:
    name: Run TPCH sf100
    needs: 
      - start-runner
      - setup-benchmarks-on-runner
    runs-on: self-hosted
    env:
      GEN: ninja
      regression_output: regression_output.txt
      
    steps:
      - name: Regression Test TPCH
        shell: bash
        run: |
          python duckdb-main/scripts/regression_test_runner.py \
            --old=duckdb-old/build/release/benchmark/benchmark_runner \
            --new=duckdb-main/build/release/benchmark/benchmark_runner \
            --benchmarks=duckdb-main/.github/regression/large.csv \
            --verbose > ${{ env.regression_output }}

      - name: Create duckdb-internal issue if faiure
        if: failure()
        shell: bash
        run: |
          # get versions
          ./duckdb-old/build/release/duckdb -c "pragma version" > old_version.txt
          ./duckdb-main/build/release/duckdb -c "pragma version" > main_version.txt
          # create body of text
          printf "\`\`\` \n Regressed Version \n$(cat main_version.txt)\nOLD VERSION \n$(cat old_version.txt) \n \`\`\` \n" > issue_body.txt
          printf "Regression Output \n \`\`\` \n $(awk '/REGRESSIONS DETECTED/,/OTHER TIMINGS/' ${{ env.regression_output }}) \n \`\`\` \n"  >> issue_body.txt
          # create issue
          gh issue create --repo ${{ env.gh_issue_repo }} --label="high priority" --title "Weekly Regression Test Failure" --body-file issue_body.txt
          # notify next run that it should not update duckdb-old
          touch previous_failed.txt 

      - name: Remove previous_failed if success.
        if: success()
        shell: bash
        run: |
          rm -f previous_failed.txt

      - name: Upload results
        if: success()
        uses: actions/upload-artifact@v3
        with:
          name: ${{ env.regression_output }}
          path: ${{ env.regression_output }}
          if-no-files-found: error

  run-micro-benchmarks:
    name: Run Micro benchmarks
    if: ${{ always() }}
    needs:
      - start-runner
      - setup-benchmarks-on-runner
      - run-large-benchmarks-on-runner
    env:
      micro_regression_output: micro_regression_output.txt 
    runs-on: self-hosted
    
    steps:
      - name: Run micro benchmarks
        shell: bash
        run: |
          python duckdb-main/scripts/regression_test_runner.py \
            --old=duckdb-old/build/release/benchmark/benchmark_runner \
            --new=duckdb-main/build/release/benchmark/benchmark_runner \
            --benchmarks=duckdb-main/.github/regression/micro.csv \
            --verbose > ${{ env.micro_regression_output }}
      
  shutdown:
    name: shut down
    if: ${{ always() }}
    needs:
      - start-runner
      - setup-benchmarks-on-runner
      - run-large-benchmarks-on-runner
      - run-micro-benchmarks

    steps:
      - name: shutdown
        shell: bash
        run: sudo shutdown
